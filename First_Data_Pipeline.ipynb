{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciewparis/repository_A/blob/main/First_Data_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtY-3Vf6E5bp"
      },
      "source": [
        "## Air Quality Pipeline\n",
        "In this notebook, we will create a pipeline that will read data from a CSV file, clean the data, and then write the cleaned data to Cloud Storeage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c_CivYaE5br"
      },
      "source": [
        "## Python Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DTPvzJixnbD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QCR7PTpxptd"
      },
      "source": [
        "## Requesting Data.\n",
        "\n",
        "For this challenge we are going to request data from the OpenAQ (Open Air Quality data) API.  \n",
        "As a Warm UP, using the üëâ [Documentation](https://docs.openaq.org/docs/introduction) and the üëâ [Official reference](https://docs.openaq.org/reference/averages_v2_get_v2_averages_get) can you find the python code to get informations about **ALL** the country available with this API ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYsHqCkCE5bs"
      },
      "outputs": [],
      "source": [
        "# Insert your code to read general informations about all the countries from the API\n",
        "# Use the request library and the /countries endpoint to do this\n",
        "\n",
        "\n",
        "#response = # Insert your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn9lQbgOE5bs"
      },
      "source": [
        "<details><summary>Solution üí™</summary>\n",
        "  <p>\n",
        "   \n",
        "  ```python\n",
        "\n",
        "  url = \"https://api.openaq.org/v2/countries?limit=1000&page=1&offset=0&sort=asc&order_by=name\"\n",
        "\n",
        "  headers = {\n",
        "      \"accept\": \"application/json\",\n",
        "      \"content-type\": \"application/json\"\n",
        "  }\n",
        "\n",
        "  response = requests.get(url, headers=headers)\n",
        "\n",
        "  print(response.json())\n",
        "  ```\n",
        "\n",
        "  </p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNvZcceyzgiv"
      },
      "outputs": [],
      "source": [
        "# Let's print this response to see how it looks like\n",
        "countries = pd.DataFrame(response.json()[\"results\"])\n",
        "print(countries.shape)\n",
        "countries"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a39XZnbzNju2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GngnxO2jE5bs"
      },
      "source": [
        "### Requesting Air Quality Data a city\n",
        "We want to create an automated pipeline that automatically download data from the last year for a specific city\n",
        "####  1 -  Date Format\n",
        "\n",
        "The expected format for the API is \"YYYY-MM-DD\", `using time.now()` create a variable `date_from` that store the date 1 year from the current date in the correct format\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLOl2ik3E5bt"
      },
      "outputs": [],
      "source": [
        "# Get Date from one year ago using datetime\n",
        "# You can use the datetime library to get the current date and time\n",
        "# and then use timedelata to subtract 365 days from it to get the date from one year ago\n",
        "# Finally, you can format the date as a string in the format \"YYYY-MM-DD\" using the .strftime method\n",
        "\n",
        "\n",
        "date_from = None #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqklVjlWAYCe"
      },
      "outputs": [],
      "source": [
        "# Run the following code without modifying it to do the api call\n",
        "\n",
        "endpoint =  \"https://api.openaq.org/v2/measurements\"\n",
        "\n",
        "params = {\n",
        "    \"date_from\": date_from,\n",
        "    \"limit\": 1000, # Keeping 1000 as the limit for speed purposes\n",
        "    \"sort\": \"desc\",\n",
        "    \"order_by\": \"datetime\",\n",
        "    \"city\": \"PARIS 9E ARRONDISSEMENT\"\n",
        "}\n",
        "\n",
        "headers = {\"accept\": \"application/json\"}\n",
        "\n",
        "response = requests.get(endpoint, headers=headers,params=params)\n",
        "\n",
        "res = pd.DataFrame(response.json()[\"results\"])\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J1-bhVP3Nc3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GlaS4alLfs1"
      },
      "source": [
        "#### 2 - Format the API result\n",
        "Notice that we have several columns that are JSON object like the `date` and `coordinates`. We could keep in this way and store them like that but to simplify the logic needed we are going to extract the information from those columns.  \n",
        "First let's extract only the `utc` from the date column.\n",
        "You can use the `pd.Series().apply` method with a lambda function for this task ! Once it's done reassign the result to the date column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VkFZVQgDKMy"
      },
      "outputs": [],
      "source": [
        "res[\"date\"] = #TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXv5x5oZE5bt"
      },
      "outputs": [],
      "source": [
        "# Run this cell to verify that the date column is a datetime object\n",
        "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
        "assert is_datetime(res[\"date\"]) == True , \"Date column is not a datetime object, maybe you should verify your conversion\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH0a-G1bE5bt"
      },
      "source": [
        "Let's do the same with the `coordinates`columns, extract the lat and lon and put them respectively in a `latitude` and `longitude` column.\n",
        "To try a different method you can use the `pd.json_normalize` function on res[\"coordinates]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wpEaVgGJefo"
      },
      "outputs": [],
      "source": [
        "# TODO Create a latitude and longitude column and drop the coordinates column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nll2LnfE5bt"
      },
      "source": [
        "Run the following column to make sure your DataFramme is correct ‚úÖ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWeIfeOBE5bt"
      },
      "outputs": [],
      "source": [
        "assert set(res.columns) == set(['locationId', 'location', 'parameter', 'value', 'date', 'unit',\n",
        "       'country', 'city', 'isMobile', 'isAnalysis', 'entity', 'sensorType',\n",
        "       'latitude', 'longitude']), \"Columns are not the same as expected\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5k-Z8DqE5bt"
      },
      "source": [
        "#### 3 - Save the Data to a Csv format\n",
        "It's alaways usefull to have a local back up of your data when you build a Data Pipeline. Let's save your DataFrame to a csv !  \n",
        "Create a data folder and then save your date in a `data/air_quality.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-z6I2i2E5bt"
      },
      "outputs": [],
      "source": [
        "# TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y47lLYcOE5bt"
      },
      "source": [
        "#### 4 - Upload Data to Cloud Storage\n",
        "\n",
        "We provide you with the python code to load your DataFrame, you can find it in the official üëâ [GCP Documentation](https://cloud.google.com/storage/docs/uploading-objects-from-memory)\n",
        "\n",
        "Before you dive in the code :\n",
        "* Go to GCP and create a new bucket. üëâ [Create a Bucket](https://console.cloud.google.com/storage/create-bucket)\n",
        "  Remember, a bucket name must be globaly unique !\n",
        "  * Choose Multi-Region -> EU\n",
        "  * Default Storage Class -> Standard\n",
        "  * Leave all the other parameters as default\n",
        "* Now, we need to create a service account to be able to load data using Python\n",
        "  * Go to the üëâ[Service account page](https://console.cloud.google.com/iam-admin/serviceaccounts/create)\n",
        "  * Name your service account `bucket-admin`\n",
        "  * Give it the role `Storage Object Admin`\n",
        "\n",
        "    <details><summary>Service Account Role</summary>\n",
        "      <p>\n",
        "\n",
        "      <img src=\"https://storage.googleapis.com/schoolofdata-images/Data-Sourcing.Data-Pipelines/service-account-role.png\"  />\n",
        "      \n",
        "      </p>\n",
        "    </details>\n",
        "\n",
        "* The final step needed is to create A JSON Key for your service account\n",
        "  * Click on freshly created service account\n",
        "  \n",
        "    <details><summary>Details</summary>\n",
        "      <p>\n",
        "\n",
        "      <img src=\"https://storage.googleapis.com/schoolofdata-images/Data-Sourcing.Data-Pipelines/click.png\"  />\n",
        "      </p>\n",
        "    </details>\n",
        "\n",
        "  * Click on KEY > ADD KEY > Create new key\n",
        "    <details><summary>Details</summary>\n",
        "      <p>\n",
        "\n",
        "      <img src=\"https://storage.googleapis.com/schoolofdata-images/Data-Sourcing.Data-Pipelines/Create%20Key.png\"  />\n",
        "      <\n",
        "      </p>\n",
        "    </details>\n",
        "\n",
        "  * Click on Create, the Private key should automatically be download on your computer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Fzn5pS0E5bu"
      },
      "outputs": [],
      "source": [
        "# If you are running this Notebook on collab, uncomment this cell to upload the key\n",
        "# from google.colab import files\n",
        "# files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYmCKzJYE5bu"
      },
      "outputs": [],
      "source": [
        "# Now we need to specify to google where the credentials are stored\n",
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"REPLACE ME WITH THE CORRECT PATH\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qit58L2XE5bu"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "# REPLACE THIS WITH YOUR OWN GOOGLE CLOUD STORAGE BUCKET NAME\n",
        "bucket_name = \"TODO:REPLACEBYCLOUDSTORAGEBUCKET\"\n",
        "# REPLACE 'storage-object-name' with the name you want the file to have in GCS\n",
        "destination_blob_name = \"storage-object-name\"\n",
        "\n",
        "\n",
        "# Leave the rest of the code as it is üòä\n",
        "from google.cloud import storage\n",
        "def upload_blob_from_memory(bucket_name, contents, destination_blob_name):\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    blob.upload_from_string(contents)\n",
        "\n",
        "    print(\n",
        "        f\"{destination_blob_name} with contents uploaded to {bucket_name}. ‚úÖ\"\n",
        "    )\n",
        "upload_blob_from_memory(bucket_name, res.to_csv(),destination_blob_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsjVJguyE5bu"
      },
      "source": [
        "Congratulations! You have successfully completed this manual pipeline notebook. This is a simple pipeline that reads data from an API, processes it, and uploads it to Google Cloud Storage. In the next step we will try to automate and schedule this process, based on your current knowledge of GCP do you know wich services we are going to use ?\n",
        "<details>\n",
        "<summary>Answer üßô‚Äç‚ôÄÔ∏è</summary>\n",
        "  <p>\n",
        "  \n",
        "  There is a lot of possibilities and not a single infrastructure solution. But we can design and build a robust pipeline using Cloud function, Cloud scheduller and Pub Sub !\n",
        "\n",
        "  </p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4drvHvKVGKSY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}